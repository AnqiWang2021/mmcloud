The script and this README were automatically generated by claude.ai based on some initial draft scripts from us.

# S3-FTP Transfer Tool

A versatile Python script for transferring files between Amazon S3 and various FTP-based protocols (FTP, FTPS, SFTP).


## Features

- **Bidirectional Transfer**: Upload from FTP to S3, or download from S3 to FTP
- **Protocol Auto-detection**: Automatically tries SFTP, FTPS, and FTP in that order
- **Protocol Selection**: Manually specify the protocol to use (FTP, FTPS, or SFTP)
- **Chunked Transfer**: Handles large files by breaking them into manageable chunks
- **Resume Support**: Skips existing files of the same size
- **Bulk Operations**: Transfer entire directories at once
- **Speed Reporting**: Reports transfer speed for each chunk
- **AWS Integration**: Works with standard AWS credentials

## Requirements

- Python 3.6+
- Required Python packages:
  - boto3
  - paramiko
  - ftplib (standard library)

## Installation

1. Clone the repository or download the script:

```bash
git clone https://github.com/yourusername/s3-ftp-transfer.git
cd s3-ftp-transfer
```

2. Install the required dependencies:

```bash
pip install boto3 paramiko
```

## Usage

The tool uses a command-line interface with various options:

```
usage: s3_ftp_transfer.py [-h] --mode {upload,download} [--protocol {auto,ftp,ftps,sftp}]
                         --ftp-host FTP_HOST [--ftp-port FTP_PORT] --ftp-user FTP_USER
                         --ftp-password FTP_PASSWORD --ftp-path FTP_PATH
                         --s3-bucket S3_BUCKET --s3-path S3_PATH
                         [--aws-access-key AWS_ACCESS_KEY]
                         [--aws-secret-key AWS_SECRET_KEY]
                         [--aws-region AWS_REGION]
                         [--chunk-size CHUNK_SIZE] [--bulk] [--recursive]
```

### Basic Examples

#### Upload a file from FTP to S3:

```bash
python s3_ftp_transfer.py --mode upload \
    --ftp-host ftp.example.com --ftp-user myuser --ftp-password mypassword \
    --ftp-path /path/to/file.txt \
    --s3-bucket my-bucket --s3-path path/to/file.txt
```

#### Download a file from S3 to FTP:

```bash
python s3_ftp_transfer.py --mode download \
    --ftp-host ftp.example.com --ftp-user myuser --ftp-password mypassword \
    --ftp-path /path/to/file.txt \
    --s3-bucket my-bucket --s3-path path/to/file.txt
```

#### Upload an entire directory from FTP to S3:

```bash
python s3_ftp_transfer.py --mode upload --bulk \
    --ftp-host ftp.example.com --ftp-user myuser --ftp-password mypassword \
    --ftp-path /path/to/directory \
    --s3-bucket my-bucket --s3-path path/in/s3
```

### Command-Line Options

| Option | Description |
|--------|-------------|
| `--mode {upload,download}` | Transfer direction: upload (FTP to S3) or download (S3 to FTP) |
| `--protocol {auto,ftp,ftps,sftp}` | FTP protocol to use (default: auto) |
| `--ftp-host` | FTP server hostname or IP |
| `--ftp-port` | FTP server port (default: 21 for FTP/FTPS, 22 for SFTP) |
| `--ftp-user` | FTP username |
| `--ftp-password` | FTP password |
| `--ftp-path` | Path on FTP server |
| `--s3-bucket` | S3 bucket name |
| `--s3-path` | Path/key prefix in S3 bucket |
| `--aws-access-key` | AWS access key ID (optional, uses AWS CLI config if not provided) |
| `--aws-secret-key` | AWS secret access key |
| `--aws-region` | AWS region name |
| `--chunk-size` | Chunk size for multipart transfers in bytes (default: 6291456 - 6MB) |
| `--bulk` | Process entire directory instead of single file |
| `--recursive` | Process directories recursively (with --bulk) |

## Configuration File

Instead of providing all parameters on the command line, you can use a configuration file:

Create a file named `config.py` in the same directory:

```python
# FTP/SFTP Configuration
FTP_HOST = "ftp.example.com"
FTP_PORT = 22  # 21 for FTP/FTPS, 22 for SFTP
FTP_USERNAME = "username"
FTP_PASSWORD = "password"

# S3 Configuration
S3_BUCKET_NAME = "my-bucket"

# Transfer options
CHUNK_SIZE = 6291456  # 6 MB
```

## How It Works

### Automatic Protocol Detection

When using `--protocol auto`, the script tries to connect using different protocols in this order:
1. SFTP (SSH File Transfer Protocol)
2. FTPS (FTP Secure - FTP over TLS/SSL)
3. FTP (File Transfer Protocol - unencrypted)

### Chunked File Transfer

For large files, the script:
1. Divides the file into chunks (default 6MB)
2. Transfers each chunk individually
3. Reports speed and progress for each chunk
4. Combines chunks at the destination

### AWS Credentials

The script uses boto3, which looks for AWS credentials in the following places:
1. Command line arguments: `--aws-access-key` and `--aws-secret-key`
2. Environment variables: `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`
3. AWS credentials file: `~/.aws/credentials`
4. IAM role for EC2 instance profiles

## Error Handling

The script includes various error handling features:
- Skips files that already exist with the same size
- Reports connection failures with clear error messages
- Aborts multipart uploads on failure to avoid orphaned parts

## Security Considerations

- Use SFTP or FTPS whenever possible rather than unencrypted FTP
- Store credentials securely (don't hardcode in scripts)
- Consider using IAM roles instead of access keys when running on AWS resources

## Limitations

- The FTP/FTPS chunk transfer requires the FTP server to support the `REST` command
- Some FTP servers may not properly report file sizes
- Very large files may require additional memory for buffering chunks

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License
